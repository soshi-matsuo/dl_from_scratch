{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNにおける学習→損失関数を最小化するようなパラメータを自動で導き出す（パーセプトロンは手動で設定した）  \n",
    "通常の機械学習手法では、データの特徴量は人間が設計する必要があるが、NNでは特徴量まで含めデータをそのまま学習する  \n",
    "これは、扱う対象のデータにかかわらず同じフローで処理ができるという利点につながる（認識対象が犬でも人でも手書き文字でも、処理の流れは変わらない）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNの性能の悪さを表すのが損失関数で、二乗和誤差や交差エントロピー誤差が主に用いられる  \n",
    "以下のケースでは二乗和誤差を計算しており、その結果が小さいほど予測値が正解に近いことを示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:05:51.866697Z",
     "start_time": "2019-02-06T09:05:51.861223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.675\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t) ** 2)\n",
    "\n",
    "# 正解ラベルが2で、予測も2\n",
    "t = np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "y = np.array([0.1, 0.05, 0.6,0,0.05,0.1,0,0.1,0,0])\n",
    "\n",
    "print(mean_squared_error(y, t))\n",
    "\n",
    "# 正解ラベルが2で、予測は6\n",
    "t = np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "y = np.array([0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0])\n",
    "\n",
    "print(mean_squared_error(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のケースでは交差エントロピー誤差を計算  \n",
    "交差エントロピー誤差では、基本的に教師データはone-hotベクトルなので、正解ラベルに対しての予測のみを考慮すればOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:05:54.484410Z",
     "start_time": "2019-02-06T09:05:54.478606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.9957302735559908\n"
     ]
    }
   ],
   "source": [
    "def cross_entopy_error(y, t):\n",
    "    # log0は-infになって正しく計算できなくなるので、微小な値をyに加える\n",
    "    delta = 1e-7\n",
    "    return - np.sum(t * np.log(y + delta))\n",
    "\n",
    "# 正解ラベルが2で、予測も2\n",
    "t = np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "y = np.array([0.1, 0.05, 0.6,0,0.05,0.1,0,0.1,0,0])\n",
    "print(cross_entopy_error(y, t))\n",
    "\n",
    "# 正解ラベルが2で、予測は6\n",
    "t = np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "y = np.array([0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0])\n",
    "print(cross_entopy_error(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習においては、すべての訓練データについて予測と損失関数を求める必要があるが、莫大な訓練データすべてについて損失関数を計算していくのは時間がかかる  \n",
    "よって、全訓練データから一部のデータを全体の近似として抽出し（ミニバッチ）、そのバッチごとに学習を行う  \n",
    "以下は、ミニバッチをMNISTから抽出している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:01.657831Z",
     "start_time": "2019-02-06T09:06:01.252358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[11366 33121  1221  4924 15796  1938 57204 57547 46298 54532]\n"
     ]
    }
   ],
   "source": [
    "from dlbook.dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "mini_batch = 10\n",
    "# np.random.choice(a, b)で、0~aの範囲の中で、b個の数字をランダムに取り出す\n",
    "batch_mask = np.random.choice(train_size, mini_batch)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交差エントロピー誤差を、ミニバッチ学習に対応するように修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:03.905631Z",
     "start_time": "2019-02-06T09:06:03.884821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.849552437273667\n",
      "0.4307827622463123\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    # 入力データが1つだけの場合も一般的に計算できるように、2次元に整形する\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    mini_batch = y.shape[0]\n",
    "    return - np.sum(t * np.log(y + 1e-7)) / mini_batch\n",
    "\n",
    "# mini_batch == 10のケース\n",
    "y = np.array([[0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0],\n",
    "            [0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0]])\n",
    "\n",
    "t = np.array([[0,0,1,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,1,0,0,0],\n",
    "             [1,0,0,0,0,0,0,0,0,0],\n",
    "             [0,0,0,0,0,1,0,0,0,0],\n",
    "             [0,1,0,0,0,0,0,0,0,0],\n",
    "             [0,0,0,1,0,0,0,0,0,0],\n",
    "             [0,0,0,0,1,0,0,0,0,0],\n",
    "             [0,0,0,0,0,0,0,1,0,0],\n",
    "             [0,0,0,0,0,0,0,0,0,1],\n",
    "             [0,0,0,0,0,0,0,0,1,0]])\n",
    "\n",
    "print(cross_entropy_error(y, t))\n",
    "\n",
    "# y.ndim == 1のケース\n",
    "y = np.array([0.1, 0.05, 0.05, 0, 0.05, 0.65, 0, 0.1, 0, 0.1])\n",
    "t = np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "print(cross_entropy_error(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習では、損失関数の「勾配」を利用してパラメータの学習を行う  \n",
    "勾配とは、ある関数について、すべての変数についての偏微分をベクトルとしてまとめたものである  \n",
    "「変数xの微小な変化に対して、関数f(x)の出力がどのくらい変化するか」を表すのが微分であり、以下の式で定義できる  \n",
    "$$\n",
    "  \\frac{d}{d x} f(x) = \\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h} \n",
    "$$\n",
    "（後述する誤差逆伝播法では、解析的に偏微分を求める）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:07.280531Z",
     "start_time": "2019-02-06T09:06:07.277472Z"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    # hは丸め誤差が生じない程度の、0に近い慣習的な値\n",
    "    h = 1e-4\n",
    "    # f(x + h)とf(x)の差分は前方差分と呼ばれるが、実装上これは誤差が大きくなる(hを0に無限に近づけられないため)\n",
    "    # よって、f(x + h)とf(x - h)の差分（中心差分）を用いて数値微分を計算する\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex.$ f(x) = 0.01x^2+0.1x $ をx=5, x=10でそれぞれ数値微分してみると、解析解0.2, 0.3とほぼ一致する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:08.993659Z",
     "start_time": "2019-02-06T09:06:08.989563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "f1 = (lambda x: 0.01 * x ** 2 + 0.1 * x)\n",
    "print(numerical_diff(f1, 5))\n",
    "print(numerical_diff(f1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある変数以外の変数を定数とみなして微分を行う手法が偏微分で、$ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1} $のように表す（それぞれ、fをx0、x1に着目して偏微分する、の意）  \n",
    "それらを$ (\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}) $ という様に、ベクトルとしてまとめてしまったものを勾配という(勾配の形状は、元のパラメータの形状と同じになる）  \n",
    "以下のように実装可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:11.483122Z",
     "start_time": "2019-02-06T09:06:11.478100Z"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # xと同じ形状の0配列を生成\n",
    "    \n",
    "    # 各々の変数について偏微分を計算\n",
    "    for idx in range(x.size):\n",
    "        # 中心差分で数値微分\n",
    "        tmp = x[idx]\n",
    "        # idx番目のxについて微小に変化させて関数を数値微分することで、idx番目のxについての偏微分が求まる\n",
    "        x[idx] = tmp + h\n",
    "        fx1 = f(x)\n",
    "        x[idx] = tmp - h\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1 - fx2) / (2 * h)\n",
    "        x[idx] = tmp # x[idx]をもとに戻す\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex.$ f(x_0,x_1) = x_0^2 + x_1^2$の勾配を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:13.900862Z",
     "start_time": "2019-02-06T09:06:13.895631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x: x[0]**2 + x[1]**2\n",
    "print(numerical_gradient(f2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(f2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(f2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、関数f2の各点における$ (x_0, x_1) $の勾配を表し、**各点において関数の値を最も減らす方向を示している**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配は、ある点においてその関数の値を最も減らす方向を表している  \n",
    "機械学習における学習とは、損失関数の値が最も小さくなるようなパラメータを導くことである  \n",
    "つまり、勾配を用いてパラメータを更新し、更新したパラメータを用いて再び損失関数＆勾配を計算するのを繰り返していくことで、損失関数を小さくしていくことができる  \n",
    "これを勾配降下法という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:16.820194Z",
     "start_time": "2019-02-06T09:06:16.813926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "def grad_descent(f, init_x, lr=0.1, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        # 勾配*学習率で減算して、パラメータxを更新する\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "# 勾配降下でf2が最小になるときのパラメータxを探索\n",
    "print(grad_descent(f2, init_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率が大きすぎor小さすぎると勾配降下法はうまくいかない(以下参照）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:19.336782Z",
     "start_time": "2019-02-06T09:06:19.326308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.99999994  3.99999992]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# ほとんどxは更新されていない\n",
    "print(grad_descent(f2, np.array([-3.0, 4.0]), lr=1e-10))\n",
    "# xが大きすぎる値に発散してしまう\n",
    "print(grad_descent(f2, np.array([-3.0, 4.0]), lr=10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNにおける学習とは、重みパラメータWについての損失関数の勾配を用いて、最適なパラメータWを導くこと  \n",
    "簡易的なNNを実装し、実際に勾配を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:06:22.191167Z",
     "start_time": "2019-02-06T09:06:22.182605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40155232  0.33531113 -1.51458275]\n",
      " [-0.06299324  1.00012669 -0.01779266]]\n"
     ]
    }
   ],
   "source": [
    "from dlbook.common.functions import softmax, cross_entropy_error\n",
    "from dlbook.common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # mean=0, sigma=1の正規分布で初期化\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:07:19.869321Z",
     "start_time": "2019-02-06T09:07:19.864029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18423747  1.1013007  -0.92476304] 1\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([0.6, 0.9])\n",
    "p = net.predict(x1)\n",
    "print(p, np.argmax(p))\n",
    "t1 = np.array([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T09:07:53.744238Z",
     "start_time": "2019-02-06T09:07:53.736970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44340935,  0.39175507,  0.05165428],\n",
       "       [-0.66511403,  0.58763261,  0.07748142]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 損失関数において、パラメータWの勾配dWを導出\n",
    "fW = lambda W: net.loss(x1, t1)\n",
    "# net.Wは参照渡しされており、ミュータブルな要素なので、numerical_gradient内での操作はnet.Wに直接反映される\n",
    "dW = numerical_gradient(fW, net.W) \n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この勾配は、各パラメータを$ h $だけ動かしたときの損失関数の動きを表している  \n",
    "ex. $ w_{11} $を$ h $だけ動かすと、損失関数の値は0.44hだけ減少する  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ミニバッチ\n",
    "    - 訓練データの中からランダムに一部のデータを抽出する  \n",
    "    - 選ばれた一部のデータ群をミニバッチと呼び、ミニバッチについて損失関数の値を減らすことを目指す\n",
    "2. 勾配の算出\n",
    "    - ミニバッチの損失関数を減少させるために、各重みパラメータの勾配を求める（損失関数を重みパラメータについて偏微分する）\n",
    "    - 勾配は、損失関数の値を最も減らす方向を示す\n",
    "3. パラメータの更新\n",
    "    - 重みパラメータを勾配方向に微小に更新する\n",
    "4. 1~3を繰り返す\n",
    "  \n",
    "これらのプロセスは、ミニバッチによって無作為抽出されたデータを用いてパラメータ更新を行っているので、確率的勾配降下法と呼ばれる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に上記のプロセスを一貫して実装し、テストデータによる評価を行う（MNISTを使う）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T01:06:34.599574Z",
     "start_time": "2019-02-07T01:06:34.590904Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dlbook.common.functions import *\n",
    "from dlbook.common.gradient import numerical_gradient\n",
    "from dlbook.dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 画像データの推論\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        # 推論と正解ラベルの誤差を計算\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        # 推論の精度を計算\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        # パラメータ群の勾配を数値微分によって計算\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値微分による勾配計算はかなり時間がかかるので、実用上は誤差逆伝播法を使う  \n",
    "また、このモデルが汎化性能を持っているかを確かめるために、1エポックごとに訓練データとテストデータの認識精度を記録する  \n",
    "（1エポック=学習において、訓練データをすべて使い切ったときのイテレーション回数。訓練データ/ミニバッチサイズで求められる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T01:48:22.939271Z",
     "start_time": "2019-02-07T01:42:39.724384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.10218333333333333, test acc: 0.101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-17dbad598248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 勾配計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# パラメータ更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4e7cc51e9d1d>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PythonCodes/dl_fscratch/dlbook/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x+h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4e7cc51e9d1d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# パラメータ群の勾配を数値微分によって計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4e7cc51e9d1d>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# 推論と正解ラベルの誤差を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PythonCodes/dl_fscratch/dlbook/common/functions.py\u001b[0m in \u001b[0;36mcross_entropy_error\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 1エポックあたりのイテレーション数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# hyper params\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(784, 10, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチ取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータ更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習曲線の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1エポックごとに精度を計算\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc: {}, test acc: {}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251.165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 510.880818,
   "position": {
    "height": "40px",
    "left": "1102.83px",
    "right": "20px",
    "top": "120px",
    "width": "407.159px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
